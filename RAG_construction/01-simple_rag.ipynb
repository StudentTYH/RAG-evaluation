{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3faffc6a",
   "metadata": {},
   "source": [
    "# 包下载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ff6efe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel,Field\n",
    "from typing import Optional,List\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate,MessagesPlaceholder\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import StateGraph, START ,END\n",
    "from langgraph.graph.message import add_messages\n",
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b47d53",
   "metadata": {},
   "source": [
    "# LLM定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dab31b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_community.llms import VLLMOpenAI\n",
    "import os\n",
    "os.environ[\"OLLAMA_HOST\"]=\"http://localhost:11450\"\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model=\"qwen2.5\",\n",
    "    temperature=0,\n",
    "    max_token = 300000,    \n",
    "    # other params...\n",
    ")\n",
    "\n",
    "llm_multimodal = ChatOllama(\n",
    "    model=\"llama3.2-vision:90b\",\n",
    "    temperature=0,\n",
    "    # other params...\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01927cb",
   "metadata": {},
   "source": [
    "# 文档读取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fec33cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import ArxivLoader\n",
    "doc = ArxivLoader(query=\"1706.03762\", load_max_docs=1).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d310123f",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_content = doc[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef03b90",
   "metadata": {},
   "source": [
    "# 读取CSV数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e7703b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "qa_data = pd.read_csv(\"../test_dataset_generation/qa_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb1f395",
   "metadata": {},
   "source": [
    "# 向量数据库构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b89d6023",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mr Tao\\AppData\\Local\\Temp\\ipykernel_4752\\3039845692.py:9: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  relevant_docs = base_retriever.get_relevant_documents(\"What is Retrieval Augmented Generation?\")\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "embeddings = OllamaEmbeddings(model=\"nomic-embed-text:latest\")\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500)\n",
    "docs = text_splitter.split_documents(doc)\n",
    "vectorstore = Chroma.from_documents(docs, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "387d2a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\" : 2})\n",
    "relevant_docs = retriever.get_relevant_documents(\"What is the transformer?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "11d20bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "template = \"\"\"You are an assistant for question-answering tasks. \n",
    "Use the following pieces of retrieved context to answer the question. \n",
    "If you don't know the answer, just say that you don't know. \n",
    "\n",
    "Question: {query} \n",
    "\n",
    "Context: {context} \n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "qa_chain = prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f04b128",
   "metadata": {},
   "source": [
    "# 评估数据构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9cc16c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = qa_data[\"question\"].tolist()\n",
    "ground_truths = qa_data[\"answer\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f3ff3ba7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from ragas import EvaluationDataset\n",
    "dataset = []\n",
    "\n",
    "def format_docs(relevant_docs):\n",
    "    return \"\\n\".join(doc.page_content for doc in relevant_docs)\n",
    "\n",
    "for query, reference in zip(questions, ground_truths):\n",
    "    relevant_docs = retriever.invoke(query)    \n",
    "    response = qa_chain.invoke({\"context\": format_docs(relevant_docs), \"query\": query})\n",
    "    dataset.append(\n",
    "        {\n",
    "            \"user_input\": str(query),\n",
    "            \"retrieved_contexts\": [str(rdoc.page_content) for rdoc in relevant_docs],\n",
    "            \"response\": str(response),\n",
    "            \"reference\": str(reference),\n",
    "        }\n",
    "    )\n",
    "\n",
    "evaluation_dataset = EvaluationDataset.from_list(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e537242",
   "metadata": {},
   "source": [
    "# 开始测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4f3b9d50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  24%|██████████████████████████████████████                                                                                                                       | 67/276 [02:35<07:46,  2.23s/it]Exception raised in Job[59]: OutputParserException(Invalid json output: {\"claims\": [\"Dot-product attention is faster than additive attention.\", \"Dot-product attention is more space-efficient than additive attention.\", \"Dot-product attention can be implemented using highly optimized matrix multiplication code.\", \"The implementation advantage of dot-product attention leads to significant performance benefits.\", \"Significant performance benefits are especially noticeable as the dimensionality \\(d_k\\) increases.\"]}\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE )\n",
      "Evaluating:  33%|███████████████████████████████████████████████████▊                                                                                                         | 91/276 [03:38<05:37,  1.82s/it]Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "Prompt claim_decomposition_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
      "Exception raised in Job[74]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
      "Evaluating:  52%|████████████████████████████████████████████████████████████████████████████████▊                                                                           | 143/276 [06:06<05:18,  2.40s/it]Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "Prompt claim_decomposition_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
      "Exception raised in Job[128]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
      "Evaluating:  53%|█████████████████████████████████████████████████████████████████████████████████▉                                                                          | 145/276 [06:10<04:37,  2.12s/it]Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "Prompt claim_decomposition_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
      "Exception raised in Job[137]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
      "Evaluating:  55%|██████████████████████████████████████████████████████████████████████████████████████▍                                                                     | 153/276 [06:39<07:58,  3.89s/it]Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "Prompt claim_decomposition_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
      "Exception raised in Job[143]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
      "Evaluating:  57%|████████████████████████████████████████████████████████████████████████████████████████▏                                                                   | 156/276 [06:55<09:13,  4.61s/it]Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "Prompt claim_decomposition_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
      "Exception raised in Job[146]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
      "Evaluating: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 276/276 [14:20<00:00,  3.12s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'context_recall': 0.8022, 'faithfulness': 0.8733, 'factual_correctness(mode=f1)': 0.4983}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ragas import evaluate\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.metrics import LLMContextRecall, Faithfulness, FactualCorrectness\n",
    "\n",
    "evaluator_llm = LangchainLLMWrapper(llm)\n",
    "\n",
    "\n",
    "result = evaluate(\n",
    "    dataset=evaluation_dataset,\n",
    "    metrics=[LLMContextRecall(), Faithfulness(), FactualCorrectness()],\n",
    "    llm=evaluator_llm,\n",
    ")\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7dbc13c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python(langchain)",
   "language": "python",
   "name": "langchain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
