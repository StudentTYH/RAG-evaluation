{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3faffc6a",
   "metadata": {},
   "source": [
    "# 包下载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbeef6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas -q -i https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "!pip install arxiv -q -i https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "!pip install pydantic -q -i https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "!pip install langgraph langchain-community langchain-ollama -q -i https://pypi.tuna.tsinghua.edu.cn/simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ff6efe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel,Field\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate,MessagesPlaceholder\n",
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from pydantic.fields import FieldInfo\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_community.llms import VLLMOpenAI\n",
    "from typing import Optional,List\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b47d53",
   "metadata": {},
   "source": [
    "# LLM定义\n",
    "此Demo需要花费大量的token，建议使用本地部署的LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dab31b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "os.environ[\"OLLAMA_HOST\"]=\"http://localhost:11450\"\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model=\"qwen2.5\",\n",
    "    temperature=0,\n",
    "    max_token = 300000,    \n",
    "    # other params...\n",
    ")\n",
    "\n",
    "llm_multimodal = ChatOllama(\n",
    "    model=\"llama3.2-vision:90b\",\n",
    "    temperature=0,\n",
    "    # other params...\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c9d718",
   "metadata": {},
   "source": [
    "# 文档读取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73ab4488",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import ArxivLoader\n",
    "doc = ArxivLoader(query=\"1706.03762\", load_max_docs=1).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28b31257",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_content = doc[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca33a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "Markdown(doc_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851625b3",
   "metadata": {},
   "source": [
    "# QA对构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5de40322",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "As a professional reader, carefully read the provided text and complete the following tasks:\n",
    "1.Generate as many questions as possible, covering different aspects such as facts, reasoning, word meaning, and structure to fully explore the understanding of the text.\n",
    "2.Use direct quotations from the original text whenever possible in your answers (you may simplify or omit parts appropriately, but do not change the meaning).\n",
    "3.Strictly follow the output format below, with one question-answer pair per line and no extra numbering or spacing:\n",
    "\n",
    "question: <your question>\n",
    "answer: <your answer>\n",
    "\n",
    "Example:\n",
    "question: How does the author describe the significance of the study?\n",
    "answer: “This study is of critical importance to...”\n",
    "\n",
    "Please follow the above instructions to complete the task.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate,MessagesPlaceholder\n",
    "paper_segment_prompt_finally = ChatPromptTemplate(\n",
    "    [\n",
    "        (\"system\",template),\n",
    "        (\"user\",\"{paper}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "rag_chain = paper_segment_prompt_finally | llm | StrOutputParser() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a883b28c",
   "metadata": {},
   "source": [
    "# 结构输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90a96d14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\python\\anaconda\\envs\\langchain_env\\Lib\\site-packages\\pydantic\\json_schema.py:2324: PydanticJsonSchemaWarning: Default value (FieldInfo(annotation=NoneType, required=False, default=None, description='question in a QA pair'),) is not JSON serializable; excluding default from JSON schema [non-serializable-default]\n",
      "  warnings.warn(message, PydanticJsonSchemaWarning)\n",
      "D:\\python\\anaconda\\envs\\langchain_env\\Lib\\site-packages\\pydantic\\json_schema.py:2324: PydanticJsonSchemaWarning: Default value (FieldInfo(annotation=NoneType, required=False, default=None, description='answer in a QA pair'),) is not JSON serializable; excluding default from JSON schema [non-serializable-default]\n",
      "  warnings.warn(message, PydanticJsonSchemaWarning)\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "\n",
    "class QA(BaseModel):\n",
    "    question: Optional[str] = Field(default=None,description=\"question in a QA pair\"),\n",
    "    answer: Optional[str] = Field(default=None,description=\"answer in a QA pair\"),\n",
    "        \n",
    "class QA_LIST(BaseModel):\n",
    "    QA_Pair : List[QA]\n",
    "\n",
    "parse = PydanticOutputParser(pydantic_object=QA_LIST)\n",
    "      \n",
    "paper_segment_prompt_finally = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"Wrap the output in `json` tags\\n{format_instructions}\",\n",
    "        ),\n",
    "        (\"human\", \"text：{paper}\"),\n",
    "    ]\n",
    ").partial(format_instructions=parse.get_format_instructions())\n",
    "\n",
    "QA_segment_llm = paper_segment_prompt_finally | llm | parse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346af14f",
   "metadata": {},
   "source": [
    "# 开始构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3783b152",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_list = []\n",
    "for i in range(0,len(doc_content),2000):\n",
    "    try:\n",
    "        aq_string = rag_chain.invoke({\"paper\":doc_content[i:i+2000]})\n",
    "        qa_pair = QA_segment_llm.invoke(aq_string)\n",
    "        for d in qa_pair.QA_Pair:\n",
    "            qa_list.append([d.question,d.answer])\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "14801a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_list_fina = []\n",
    "for qa in qa_list:\n",
    "    if not isinstance(qa[0],tuple):\n",
    "        qa_list_fina.append(qa)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e286ef18",
   "metadata": {},
   "source": [
    "# 保存为CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0e7703b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_qa = pd.DataFrame(qa_list_fina,columns=[\"question\",\"answer\"])\n",
    "df_qa.to_csv(\"qa_data.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python(langchain)",
   "language": "python",
   "name": "langchain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
